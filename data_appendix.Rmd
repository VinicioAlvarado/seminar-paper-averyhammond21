---
title: "Data Appendix to \"Name of your paper\""
author: "Avery Hammond"
output: 
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, echo = F, message = F}
knitr::opts_chunk$set(results = 'asis', cache = T)
library(tidyverse)
library(summarytools)
library(sf)
library(stargazer)

systemInfo <- Sys.info()
if (systemInfo[[1]] == "Windows") {
  run_summary <- T
} else {
    run_summary <- F}

if (run_summary) {
  st_options(plain.ascii = F,
             style = "rmarkdown",
             footnote = NA,
             subtitle.emphasis = F,
             dfSummary.silent = T,
             dfSummary.valid.col = F,
             dfSummary.style = "grid")
  
  #The following custom function simplifies the process of writing dfSummaries to html files
  export_summary_table <- function(dfSummary_output){
    data_info <- attr(dfSummary_output, "data_info")
    ds_name <- data_info$Data.frame
    print(dfSummary_output,
          file = file.path("output", str_c(ds_name, "_summary.html")),
          method = "browser",
          report.title = ds_name)
  }
}
```

```{r set dfSummary css style, echo = F, include = F, eval = run_summary}
st_css()
```
# Appendix description
*Your Data Appendix should begin with a brief statement explaining its purpose like the following one.*

This Data Appendix documents the data used in "Paper Title". It was prepared in a Rmarkdown document that contains both the documentation and the R code used to prepare the data used in the final estimation. It also includes descriptive statistics for both the original data and the final dataset, with a discussion of any issues of note.

The datasets used directly by the final analysis are saved in `processed-data/` at the end of this file.

*Note: this document structure will require you to re-run steps of your analysis multiple times. If your code takes a long time, please come talk with me about strategies to reduce run time or save earlier results.*

# Instructions for Use
This document includes instructions for how to create your Data Appendix. Outside of this section, instruction paragraphs are listed in *italics* (like the first paragraph above). Instructions should be removed before submission.

To start creating your own data appendix, follow these steps: 

1. Replace the title and author in the section at the top of the file (called the YAML).
1. Commit your changes with a message like "customizing data appendix".
1. Delete this instruction section of the document.
1. Remove any other instructions in italics and examples from the completed sections of the document.

Remember that you will submit your assignment by committing and then pushing your versions to your repository. I encourage you to commit your changes often as you work, but there are three specific points at which you need to both submit and push changes, corresponding to course deadlines:

1. You must submit a version with the original data section completed by the Data Appendix 1 deadline. This will include the .Rmd file, the .pdf file, and the html data summary files stored in the output folder.
2. You must submit a version with all parts completed by the Data Appendix 2 deadline.  
3. You must submit a final version of this document that is consistent with your final paper by the final project deadline.  

While creating your data appendix, refer regularly to the assignment descriptions posted on Moodle.

A few tips:

- When creating a list like this one, be sure to put an empty line above the list. If you don't do this, your entries won't be formatted a list.
- Make sure you have empty lines above and below section and subsection headings.
- When creating numbered lists, you can number all items in your list with 1. Rmarkdown will number them sequentially when it creates your final document.

#3 Raw data
*Each dataset you use will have its own documentation section. The next subsection in this document (Dataset description) is a template. You can copy this section and paste it into your document each time you need to add a section for a new dataset. Note that each line in the Dataset description section __must__ end with two spaces.* 
This section documents the datasets used in this analysis.

##3.1 Historical Pollutant and Weather Data
**Citation:** Texas Commission on Environmental Quality. (2003-2004). "Historical Pollutant and Weather Data." Retrieved from https://www.tceq.texas.gov/airquality/monops/historical_data.html#red 
**Date Downloaded:** March 10, 2020 
**Filename(s):**
raw_data/camswx_200x.csv
raw_data/co_200x.csv
raw_data/nox_200x.csv
raw_data/oz_200x.csv
raw_data/pm25x_200x.csv
raw_data/so2_200x.csv
*If you have a large number of files you can use a patten (see visit data below)*
**Unit of observation:** parts per billion, meters/second, degrees compass, decrees Celsius
**Dates covered:** 01/01/2003-12/31/2004

### To obtain a copy

Interested users should visit the Historical Pollutant and Weather Data on the Texas Commission on Environmental Quality website at https://www.tceq.texas.gov/airquality/monops/historical_data.html#red. To download data from 2003 and 2004, users should click on the corresponding years for each column, which will download the data setfor each column as a CSV file.

### Variable descriptions

Create a bullet list with the name of each variable in the dataset followed by any information the user would need to understand it.

- **date:** The date of the observation.
- **hour:** The hour of the observation.
- **AQCR:** Air Quality Control Region code identifying a group of counties that share geographical or pollutant concentration characteristics based on a common pollutant source.
- **ozone:** Concentration of ozone in parts per billion.
- **nitrogen oxides:** Concentration of nitrogen oxides in parts per billion.
- **carbon monoxide:** Concentration of carbon monoxide in parts per billion.
- **sulfur dioxide:** Concentration of sulfur dioxide in parts per billion.
- **PM2.5:** Concentration of particulate matter 2.5 in parts per billion.
- **TMP1:** Temperature taken hourly, measured in degrees celsius.
- **WDR1:** Wind direction taken hourly, measured in degrees compass.
- **WSR1:** Wind speed taken hourly, measured in meters per second. 

### Data import code and summary

```{r set up download and read function, echo = T}

years = c("2003","2004")

download_and_read_data <- function(filename){
  
  if (!file.exists(file.path("raw-data",str_c(filename, ".csv")))) {
    
    destfile = file.path("raw-data",str_c(filename, ".zip"))
    
    download.file(url = str_c("https://www.tceq.texas.gov/assets/public/compliance/monops/air/ozonehist/", filename, ".zip"), destfile = destfile)
    
    unzip(destfile, exdir = "raw-data", junkpaths = T)
  }
  this_data <- read_csv(file.path("raw-data", str_c(filename, ".csv")))
}
```

```{r read in ozone data, echo = T}
oz_data <- lapply(str_c("oz_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(airs, ST_CODE, AQCR, date, contains("OZ1hr")) %>%
  select(-OZ1hrvh, -OZ1hrvd, -OZ1hrpk, -OZ1hrav) 

if (run_summary) {
  export_summary_table(dfSummary(oz_data))
} 

oz_hourly <- oz_data %>% 
  group_by(airs, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("OZ1"), names_to = "hour", values_to = "ozone") %>%
  mutate(hour = as.numeric(str_remove(hour,"OZ1hr"))) %>%
  mutate(Date = as.Date(date, "%m/%d/%Y")) %>%
  mutate(after = Date >="2004/01/01") #JANUARY 1 CHANGE AND CODE CHANGE
```

```{r read in co data, echo = T}
co_data <- lapply(str_c("co_",years), download_and_read_data) %>% 
  bind_rows() 

if (run_summary) {
  export_summary_table(dfSummary(co_data))
}

co_hourly <- co_data %>% 
  select(airs, ST_CODE, AQCR, date, contains("CO1hr")) %>%
  select(-CO1hrvh, -CO1hrvd, -CO1hrpk, -CO1hrav) %>%
  group_by(airs, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("CO1"), names_to = "hour", values_to = "carbon_monoxide") %>%
  mutate(hour = as.numeric(str_remove(hour,"CO1hr"))) %>%
  mutate(Date = as.Date(date, "%m/%d/%Y")) %>%
  mutate(after = Date >="2004/01/01")

```

```{r read in so2 data, echo = T}
so2_data <- lapply(str_c("so2_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(airs, ST_CODE, AQCR, date, contains("SO21hr")) %>%
  select(-SO21hrvh, -SO21hrvd, -SO21hrpk, -SO21hrav) 

if (run_summary) {
  export_summary_table(dfSummary(so2_data))
}

so2_hourly <- so2_data %>%
  group_by(airs, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("SO21"), names_to = "hour", values_to = "sulfur_dioxide") %>%
  mutate(hour = as.numeric(str_remove(hour,"SO21hr"))) %>%
  mutate(Date = as.Date(date, "%m/%d/%Y")) %>%
  mutate(after = Date >="2004/01/01")

```

```{r read in nox data, echo = T}
nox_data <- lapply(str_c("nox_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(airs, ST_CODE, AQCR, date, contains("NOX1hr")) %>%
  select(-NOX1hrvh, -NOX1hrvd, -NOX1hrpk, -NOX1hrav) 

if (run_summary) {
  export_summary_table(dfSummary(nox_data))
}

nox_hourly <- nox_data %>%
  group_by(airs, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("NOX1"), names_to = "hour", values_to = "nitrogen_oxides") %>%
  mutate(hour = as.numeric(str_remove(hour,"NOX1hr"))) %>%
  mutate(Date = as.Date(date, "%m/%d/%Y")) %>%
  mutate(after = Date >="2004/01/01")

```

```{r read in pm25 data, echo = T}
pm25_data <- lapply(str_c("pm25x_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(AIRS, ST_CODE, AQCR, date, contains("PM251hr")) %>%
  select(-PM251hrvh, -PM251hrvd, -PM251hrpk, -PM251hrav) 

if (run_summary) {
  export_summary_table(dfSummary(pm25_data))
}

pm25_hourly <- pm25_data %>%
  group_by(AIRS, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("PM251"), names_to = "hour", values_to = "PM2_5") %>%
  mutate(hour = as.numeric(str_remove(hour,"PM251hr"))) %>%
  mutate(Date = as.Date(date, "%m/%d/%Y")) %>%
  mutate(after = Date >="2004/01/01")

```

```{r read in weather data, echo = T}
weather_data <- lapply(str_c("camswx_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(AIRS, ST_CODE, AQCR, date, contains("WSR1hr"), contains("TMP1hr"), contains("WDR1hr"), contains("WHR1hr")) %>%
  select(-contains("hrvh"), -contains("hrvd"), -contains("hrav"), -contains("hrpk")) 

if (run_summary) {
  export_summary_table(dfSummary(weather_data))
}

weather_hourly <- weather_data %>%
  group_by(AIRS, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("hr"), 
               names_to = c("measurement", "hour"),
               names_pattern = "(.*)hr(.*)",
               values_to = "value") %>% 
  mutate(hour = as.numeric(hour)) %>% 
  pivot_wider(names_from = "measurement", values_from = "value") %>%
  mutate(Date = as.Date(date, "%m/%d/%Y")) %>%
  mutate(after = Date >="2004/01/01") %>%
  mutate(airs = AIRS)

```

##3.2 Metro Rail Station Data

**Citation:** City of Houston. (2019). "Metro Rail Station (current)." Retrieved from https://cohgis-mycity.opendata.arcgis.com/datasets/coh-metro-rail-station-current-1?geometry=-95.616%2C29.649%2C-95.181%2C29.858
**Date Downloaded:** March 10, 2020
**Filename(s):** raw_data/COH_METRO_RAIL_STATION_current.csv
**Unit of observation:** degrees longitude, degrees latitude, year in service
**Dates covered:** 2004

### To obtain a copy

Interested users should visit the Metro Rail Stations section of the City of Houston GIS data website at https://cohgis-mycity.opendata.arcgis.com/datasets/1dc7a23374ac44cdae8553044bfeaf22_14?geometry=-95.618%2C29.649%2C-95.179%2C29.858. Users should select "Download," and then select "Spreadsheet" beneath "Full Dataset," which will download the metro rail station data as a csv file. 

### Variable descriptions

Create a bullet list with the name of each variable in the dataset followed by any information the user would need to understand it.

- **latutide:** The latitudinal coordinates of the corresponding station. 
- **longitude:** The longitudinal coordinates of the corresponding station.
- **Stat_Name:** The name of the metro station.
- **Stat_Loc:** The street address of the metro station.
- **STATUS:** The operative status of the metro station, either existing or non-existing.
- **service_year:** The year the metro station began operating.


### Data import code and summary

```{r read in metro data}
metro_data <- read_csv("raw-data/COH_METRO_RAIL_STATION_current.csv") %>% 
  rename(latitude = Y, longitude = X)

metro_data_geometry <- st_as_sf(metro_data, coords = c("longitude", "latitude"),
                                crs = 4326) %>% 
  filter(Label == 1) %>% 
  select(OBJECTID)

colnames(metro_data) [1] <- "latitude"

colnames(metro_data) [2] <- "longitude"

colnames(metro_data) [9] <- "service_year"

# Note: most stations appear to be in the data twice, once with label = 1, once with label = 0
# The two rows have slightly different lat/long characteristics. It might be worth checking
#the data to see what this might mean and which one you really want.

```

```{r read-in monitor location data}
monitor_locations <- readxl::read_xls("raw-data/sitefile.xls") %>% 
  mutate(longitude = -LONG, latitude = LATT) %>% 
  select(AIRS,TNRCCRNM, longitude, latitude) %>% 
  filter(TNRCCRNM == "Houston") %>% 
  filter(!is.na(longitude)) %>% 
  filter(longitude != 0) %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)

```


*While it will make your resulting file long, you should not modify the chunk options to suppress printing of code and output. I would likely not include this in the documentation for an actual paper I was submitting, but including them here will let me read your code and the output message from R and may help identify data import concerns early in the process. Since these files will exist only electronically, their length is less of a concern. If you like to print out files to proofread and want me to help you shorten the printed versions, let me know. We can temporarily modify the chunk options for printing and restore them before you submit the assignment.*


# Data Processing and Combination
*This section should include a discussion of the processing and merging steps needed to create your basic data. The code to implement these steps should be included in chunks in this section. Once the final merged data has been created, you should use the dfSummary function again to summarize the data you will be using. You should also save a file containing all the objects you will use in your final analysis to the processed_data folder.*

```{r compute distances between monitors}
distances <- st_distance(monitor_locations, metro_data_geometry) #takes all monitor locations and all metro station locations and creates a matrix
colnames(distances) <- metro_data_geometry$OBJECTID 
distance_data <- as_tibble(distances)
distance_data$airs <- monitor_locations$AIRS 

distance_list <- distance_data %>% #instead of it being a matrix, we made it into a long list
  pivot_longer(-airs, names_to = "metro_object", values_to = "distance") %>% 
  mutate(distance = as.numeric(distance/1000)) %>%
  mutate(airs = as.numeric(airs))

distance_measure <- distance_list %>% #now use this data frame
  group_by(airs) %>%
  summarize(min_dist = min(distance)) %>%
  mutate(airs = as.numeric(airs),
         exposed = min_dist <= 10.5) #can add unexposed control variable if I want
```

# Analysis Variables

This section should include a description of all the variables that are used in your final analysis. At the end of the section, you should save all of these variables in the processed_data folder of your repository.

# Discussion of Data

*This section should include a discussion of any data patterns you notice based on the summaries created in the code above.*

```{r making oz_regression_data}
oz_regression_data <- merge(distance_measure, merge(oz_hourly, weather_hourly))

# works with exposed = 6km
```

```{r running oz regression}

oz_regression_results <- lm(ozone ~ after + exposed + exposed*after + as.factor(hour) + TMP1 + WSR1 + WDR1, data = oz_regression_data)

stargazer(oz_regression_results, type = "text",
          report = 'vctp',
          intercept.bottom = F,
          header = F,
          digits = 5)
```


```{r making co_regression_data}
co_regression_data <- merge(distance_measure, merge(co_hourly, weather_hourly))

# doesn't work w exposed = 9km
```

```{r running co regression}

co_regression_results <- lm(carbon_monoxide ~ after + exposed + exposed*after + as.factor(hour) + TMP1 + WSR1 + WDR1, 
                         data = co_regression_data)

stargazer(co_regression_results, type = "text",
          report = 'vctp',
          intercept.bottom = F,
          header = F,
          digits = 5)
```

```{r making nox_regression_data}
nox_regression_data <- merge(distance_measure, merge(nox_hourly, weather_hourly))

# work w exposed = 10km
```

```{r running nox regression}

nox_regression_results <- lm(nitrous_oxides ~ after + exposed + exposed*after + as.factor(hour) + TMP1 + WSR1 + WDR1, 
                         data = nox_regression_data)

stargazer(nox_regression_results, type = "text",
          report = 'vctp',
          intercept.bottom = F,
          header = F,
          digits = 5)
```

```{r making pm25_regression_data}
pm25_regression_data <- merge(distance_measure, merge(pm25_hourly, weather_hourly))

# doesn't work w exposed = 9km
```

```{r running PM2.5 regression}

pm25_regression_results <- lm(PM2_5 ~ after + exposed + exposed*after + as.factor(hour) + TMP1 + WSR1 + WDR1, 
                         data = pm25_regression_data)

stargazer(pm25_regression_results, type = "text",
          report = 'vctp',
          intercept.bottom = F,
          header = F,
          digits = 5)
```

```{r making so2_regression_data}
so2_regression_data <- merge(distance_measure, merge(so2_hourly, weather_hourly))

# works w exposed = 6km
```

```{r running so2 regression}

so2_regression_results <- lm(sulfur_dioxide ~ exposed + after + after*exposed + as.factor(hour) + TMP1 + WSR1 + WDR1, 
                         data = so2_regression_data) 

stargazer(so2_regression_results, type = "text",
          report = 'vctp',
          intercept.bottom = F,
          header = F,
          digits = 5)
```



```{r}
save("nox_regression_data", "co_regression_data", "oz_regression_data", "pm25_regression_data", "so2_regression_data",  file = "processed-data/analysis_data.RData")
```

