---
title: "Data Appendix to \"Name of your paper\""
author: "Avery Hammond"
output: 
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, echo = F, message = F}
knitr::opts_chunk$set(results = 'asis', cache = F)
library(tidyverse)
library(summarytools)
st_options(plain.ascii = F,
           style = "rmarkdown",
           footnote = NA,
           subtitle.emphasis = F,
           dfSummary.silent = T,
           dfSummary.valid.col = F,
           tmp.img.dir = "tmp",
           dfSummary.style = "grid",
           use.x11 = F)

#The following custom function simplifies the process of writing dfSummaries to html files
export_summary_table <- function(dfSummary_output){
  data_info <- attr(dfSummary_output, "data_info")
  ds_name <- data_info$Data.frame
  print(dfSummary_output %>% st_options(use.x11 = F),
      file = file.path("output", str_c(ds_name, "_summary.html")),
      method = "browser",
      report.title = ds_name)
}
```

```{r set dfSummary css style, echo = F, include = F}
st_css()
```
# Appendix description
*Your Data Appendix should begin with a brief statement explaining its purpose like the following one.*

This Data Appendix documents the data used in "Papter Title". It was prepared in a Rmarkdown document that contains both the documentation and the R code used to prepare the data used in the final estimation. It also includes descriptive statistics for both the original data and the final dataset, with a discussion of any issues of note.

The datasets used directly by the final analysis are saved in `processed-data/` at the end of this file.

*Note: this document structure will require you to re-run steps of your analysis multiple times. If your code takes a long time, please come talk with me about strategies to reduce run time or save earlier results.*

# Instructions for Use
This document includes instructions for how to create your Data Appendix. Outside of this section, instruction paragraphs are listed in *italics* (like the first paragraph above). Instructions should be removed before submission.

To start creating your own data appendix, follow these steps: 

1. Replace the title and author in the section at the top of the file (called the YAML).
1. Commit your changes with a message like "customizing data appendix".
1. Delete this instruction section of the document.
1. Remove any other instructions in italics and examples from the completed sections of the document.

Remember that you will submit your assignment by committing and then pushing your versions to your repository. I encourage you to commit your changes often as you work, but there are three specific points at which you need to both submit and push changes, corresponding to course deadlines:

1. You must submit a version with the original data section completed by the Data Appendix 1 deadline. This will include the .Rmd file, the .pdf file, and the html data summary files stored in the output folder.
2. You must submit a version with all parts completed by the Data Appendix 2 deadline.  
3. You must submit a final version of this document that is consistent with your final paper by the final project deadline.  

While creating your data appendix, refer regularly to the assignment descriptions posted on Moodle.

A few tips:

- When creating a list like this one, be sure to put an empty line above the list. If you don't do this, your entries won't be formatted a list.
- Make sure you have empty lines above and below section and subsection headings.
- When creating numbered lists, you can number all items in your list with 1. Rmarkdown will number them sequentially when it creates your final document.

#3 Raw data
*Each dataset you use will have its own documentation section. The next subsection in this document (Dataset description) is a template. You can copy this section and paste it into your document each time you need to add a section for a new dataset. Note that each line in the Dataset description section __must__ end with two spaces.* 
This section documents the datasets used in this analysis.

##3.1 Historical Pollutant and Weather Data
**Citation:** Texas Commission on Environmental Quality. (2003-2004). "Historical Pollutant and Weather Data." Retrieved from https://www.tceq.texas.gov/airquality/monops/historical_data.html#red 
**Date Downloaded:** March 10, 2020 
**Filename(s):**
raw_data/camswx_200x.csv
raw_data/co_200x.csv
raw_data/nox_200x.csv
raw_data/oz_200x.csv
raw_data/pm25x_200x.csv
raw_data/so2_200x.csv
*If you have a large number of files you can use a patten (see visit data below)*
**Unit of observation:** parts per billion, meters/second, degrees compass, decrees Celsius
**Dates covered:** 01/01/2003-12/31/2004

### To obtain a copy

Interested users should visit the Historical Pollutant and Weather Data on the Texas Commission on Environmental Quality website at https://www.tceq.texas.gov/airquality/monops/historical_data.html#red. To download data from 2003 and 2004, users should click on the corresponding years for each column, which will download the data setfor each column as a CSV file.

##3.2 Metro Rail Station Data

**Citation:** City of Houston. (2019). "Metro Rail Station (current)." Retrieved from https://cohgis-mycity.opendata.arcgis.com/datasets/coh-metro-rail-station-current-1?geometry=-95.616%2C29.649%2C-95.181%2C29.858
**Date Downloaded:** March 10, 2020
**Filename(s):** raw_data/COH_METRO_RAIL_STATION_current.csv
**Unit of observation:** degrees longitude, degrees latitude, year in service
**Dates covered:** 2004

### To obtain a copy

Interested users should visit the Metro Rail Stations section of the City of Houston GIS data website at https://cohgis-mycity.opendata.arcgis.com/datasets/1dc7a23374ac44cdae8553044bfeaf22_14?geometry=-95.618%2C29.649%2C-95.179%2C29.858. Users should select "Download," and then select "Spreadsheet" beneath "Full Dataset," which will download the metro rail station data as a csv file. 

### Variable descriptions

Create a bullet list with the name of each variable in the dataset followed by any information the user would need to understand it.

- **variable_name:** Variable description. 
- **variable_name2:** Description of second variable.

### Data import code and summary

*Once you've described the variables, enter an R chunk by selecting Code -> Insert Chunk, or Ctrl+Alt+I, give it a name to describe the dataset you are importing. After importing, export a dataframe summary using the command.*
```{r set up download and read function}

years = c("2003","2004")

download_and_read_data <- function(filename){
  
  if (!file.exists(file.path("raw-data",str_c(filename, ".csv")))) {
    
    destfile = file.path("raw-data",str_c(filename, ".zip"))
    
    download.file(url = str_c("https://www.tceq.texas.gov/assets/public/compliance/monops/air/ozonehist/", filename, ".zip"), destfile = destfile)
    
    unzip(destfile, exdir = "raw-data", junkpaths = T)
  }
  this_data <- read_csv(file.path("raw-data", str_c(filename, ".csv")))
}
```

```{r read in ozone data}
oz_data <- lapply(str_c("oz_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(airs, ST_CODE, AQCR, date, contains("OZ1hr")) %>%
  select(-OZ1hrvh, -OZ1hrvd, -OZ1hrpk, -OZ1hrav) %>%
  group_by(airs, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("OZ1"), names_to = "hour", values_to = "ozone")

stargazer::stargazer(oz_data)

```

```{r read in co data}
co_data <- lapply(str_c("co_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(airs, ST_CODE, AQCR, date, contains("CO1hr")) %>%
  select(-CO1hrvh, -CO1hrvd, -CO1hrpk, -CO1hrav) %>%
  group_by(airs, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("CO1"), names_to = "hour", values_to = "carbon monoxide")

stargazer::stargazer(co_data)

```

```{r read in so2 data}
so2_data <- lapply(str_c("so2_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(airs, ST_CODE, AQCR, date, contains("SO21hr")) %>%
  select(-SO21hrvh, -SO21hrvd, -SO21hrpk, -SO21hrav) %>%
  group_by(airs, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("SO21"), names_to = "hour", values_to = "sulfur dioxide")

stargazer::stargazer(so2_data)

```

```{r read in nox data}
nox_data <- lapply(str_c("nox_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(airs, ST_CODE, AQCR, date, contains("NOX1hr")) %>%
  select(-NOX1hrvh, -NOX1hrvd, -NOX1hrpk, -NOX1hrav) %>%
  group_by(airs, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("NOX1"), names_to = "hour", values_to = "nitrous oxides")

stargazer::stargazer(nox_data)

```

```{r read in pm25 data}
pm25_data <- lapply(str_c("pm25x_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(AIRS, ST_CODE, AQCR, date, contains("PM251hr")) %>%
  select(-PM251hrvh, -PM251hrvd, -PM251hrpk, -PM251hrav) %>%
  group_by(AIRS, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("PM251"), names_to = "hour", values_to = "PM2.5")

stargazer::stargazer(pm25_data)
```

```{r read in weather data}
weather_data <- lapply(str_c("camswx_",years), download_and_read_data) %>% 
  bind_rows() %>%
  select(AIRS, ST_CODE, AQCR, date, contains("WSR1hr"), contains("TMP1hr")) %>%
  select(-WSR1hrvh, -WSR1hrvd, -WSR1hrpk, -WSR1hrav) %>%
  group_by(AIRS, ST_CODE, AQCR, date) %>%
  pivot_longer(cols = contains("WSR1"), names_to = "hour", values_to = "weather")

stargazer::stargazer(weather_data)
```

`export_summary_table(dfSummary(dataset_name))`

*While it will make your resulting file long, you should not modify the chunk options to suppress printing of code and output. I would likely not include this in the documentation for an actual paper I was submitting, but including them here will let me read your code and the output message from R and may help identify data import concerns early in the process. Since these files will exist only electronically, their length is less of a concern. If you like to print out files to proofread and want me to help you shorten the printed versions, let me know. We can temporarily modify the chunk options for printing and restore them before you submit the assignment.*


# Data Processing and Combination
*This section should include a discussion of the processing and merging steps needed to create your basic data. The code to implement these steps should be included in chunks in this section. Once the final merged data has been created, you should use the dfSummary function again to summarize the data you will be using. You should also save a file containing all the objects you will use in your final analysis to the processed_data folder.*

# Analysis Variables

This section should include a description of all the variables that are used in your final analysis. At the end of the section, you should save all of these variables in the processed_data folder of your repository.

# Discussion of Data

*This section should include a discussion of any data patterns you notice based on the summaries created in the code above.*
